- incom domu (categoria) a zużycie 
housing["income_cat"] = pd.cut(housing["median_income"],
                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
                               labels=[1, 2, 3, 4, 5])
housing["income_cat"].hist()


czy warto robić temp split 


Dane :
- godinne/dzienne 
- avg/ per house

- pogodowa wartości: himi, aparent temp, sunny or cloudy
- income bracket 
- holydays 

- kilka poprzednich dni
- zmiana z poprzedniego dania 

-polynomial

** WAve net model jest na stridach nie na dialcji 
** LSTM tylko unrolowane dziłają co zajmuje mnustwo pamieci 

*?* Exponential smothing fiting online initial params done only alfa beta gamma phi to choose 

avg:
	- godzinne
		* pogodowa wartości: himi, aparent temp, sunny or cloudy
		* holydays 
		* kilka poprzednich godzin
		* zmiana z poprzednich godzin
		* polynomial
		* info o procencie dnia 
		* info o procencie roku 
	? dzienne 
		* pogodowa wartości: himi, aparent temp, sunny or cloudy
		* holydays 
		* kilka poprzednich dni
			* avg energy
			* temp
		* zmiana z poprzedniego dania 
		* polynomial
per house:
	* dane z jedego domu
	* danych ze wszystkich domów
	* godzinne
		* pogodowa wartości: himi, aparent temp, sunny or cloudy
		* holydays 
		* kilka poprzednich godzin
		* zmiana z poprzedniego godzin 
		* income bracket 
		* polynomial
	? dzienne 
		* pogodowa wartości: himi, aparent temp, sunny or cloudy
		* holydays 
		- kilka poprzednich dni
		- zmiana z poprzedniego dania  
		- income bracket 
		* polynomial


przy testach feateher jako zapisyeanie danych 

+ test best models r2
+ std na całych danych -> feather 
+ wiecej epoch tak do 100
+ test clean(all) 
+ test avg models with no avg random data 
+ test avg models with no avg data for one house

+ test dla all dla jendej epoki ale co batch sprawdzenie mes i wykresik 
+ zjednoczenie danych zeby mialy taki sam wymiar ze by mogżna bylo z modelu avg an inny testować 

+ test danych nie std Owanych 
+ test svg danych nie std Owanych 

+test groupby hour, i precentage day max dla tej godiny 
+test groupby month, i precentage day max dla tej godiny 

+ add previous day energy, not only previous 30 min 

+ drop values that are nan or 0 for pastET

+ test avg calc 
+ fix year precent 
+ save best data 

+ sklearn.linear_model.PassiveAggressiveRegressor

+ GridSearch SDGReg 

+ nural network :
	+ ilczba warstw
	+ liczba neurownów
	+ aktywacje neurownów
	+ optimizers 

+test dla przewidywania na dłuższy czas niz jedne tick (temperature przyjac ze jest dana)

-test modeli na danych z generora dla porónania z pracy 

-test layer = keras.layers.Dense(10, activation="selu",
                           kernel_initializer="lecun_normal")

- time series z danych london
	+ jako jeden dom  z różnymi okresami
	+ jako kilka domoów
	- całe
- time serires a różńic miedzy poszczegulnymi wartościami danych 
	+ jako jeden dom  z różnymi okresami
	+ jako kilka domoów
	- całe
- time serires plus temp day per, Acorn_category i takie tam
	+ other dodane jako time series NIEEE to nie ma sensu
	+ other dodane tylko dla wartosci do prezwidzenia

+ LSTM
+ GRU
+ Convolv 1D + GRU
+wave NEt 
+ dropout
- Learining shedule

-czy nie warto średniej z kilku godzin (mniejsza dokłądnosć przewidywania)

- bład liczony jako cala energia w okresie przewidzianym do calej energi rzeczywistej 
- box cox 
- averave on 3,4,6 hours

-uzupełnienie danych tak aby okers 48 sie zgadzał 


- Guru net wit 48 samples ( 1 day )

- wybrać najlepszy 

    result.append("LinearRegression")
    result.append("BayesianRidge")
    result.append("ARDRegression")
    result.append("ElasticNet")
    result.append("HuberRegressor")
    result.append("Lasso")
    result.append("LassoLars")
    result.append("Rigid")
    result.append("SGDRegressor")
    result.append("SVR")
    result.append("MLPClassifier")
    result.append("KNeighborsClassifier")
    result.append("SVC")
    result.append("GaussianProcessClassifier")
    result.append("DecisionTreeClassifier")
    result.append("RandomForestClassifier")
    result.append("AdaBoostClassifier")
    result.append("GaussianNB")
    result.append("LogisticRegression")
    result.append("QuadraticDiscriminantAnalysis")

AVG all (pastET, prec itd) 
no test batches clean
SDGReg All             mean_squared_error : 0.04062475602812025
SDGReg All                       r2_score : 0.538154835684747
xgbReg All             mean_squared_error : 0.05797165054226861
xgbReg All                       r2_score : 0.34094554434277224
house 2
SDGReg All             mean_squared_error : 0.03695597423797931
SDGReg All                       r2_score : 0.39404840026221843
xgbReg All             mean_squared_error : 0.0420294925570488
xgbReg All                       r2_score : 0.31086006703838776
house 69
SDGReg All             mean_squared_error : 0.011748068018625041
SDGReg All                       r2_score : 0.3836584279172063
xgbReg All             mean_squared_error : 0.010143068619072437
xgbReg All                       r2_score : 0.46786184583578005


model sdg on all data hh clean (pastET ,sin cos prec) (2,4,6,8,10) epoch 

|████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00%
 model all             mean_squared_error : 0.030475516489953156
 model all                       r2_score : 0.6535371212776762
|████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00%
 model all             mean_squared_error : 0.03078361484904645
 model all                       r2_score : 0.6500344851712044
|████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00%
 model all             mean_squared_error : 0.030560868932605855
 model all                       r2_score : 0.6525667865174045
|████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00%
 model all             mean_squared_error : 0.03080750367767318
 model all                       r2_score : 0.6497629034791255
|████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00%
 model all             mean_squared_error : 0.030492074859774912
 model all                       r2_score : 0.6533488763802588

nodel PassiveAggressiveRegressor  on all data hh clean (pastET ,sin cos prec) (1,2,3,4,5) epoch 
 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00%
 model all             mean_squared_error : 0.03620858810929379
 model all                       r2_score : 0.5883603260685506
|████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00%
 model all             mean_squared_error : 0.06836522634033888
 model all                       r2_score : 0.2227855061886942
|████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00%
 model all             mean_squared_error : 0.03874041840708828
 model all                       r2_score : 0.5595770497063777
|████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00%
 model all             mean_squared_error : 0.03968108328142176
 model all                       r2_score : 0.5488830402912463
|████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00%
 model all             mean_squared_error : 0.05161898544776712
 model all                       r2_score : 0.4131662280160219

techniczeni ale praktycznie to na - był r2
  MLReg all             mean_squared_error : 0.02843914660204237
 MLReg all                       r2_score : 0.6766877239505339


 
temperature	holiday_ind	Acorn_category	dayPrecent	yearPrecent	pastET	energy_0	energy_1	energy_2	energy_3	energy
temperature	1.000000	-0.015529	0.000113	-0.154970	-0.084550	-0.073074	-0.078214	-0.080345	-0.082651	-0.085751	-0.077516
holiday_ind	-0.015529	1.000000	0.000210	-0.000846	-0.013170	0.002810	0.002464	0.002604	0.002753	0.002454	0.002171
Acorn_category	0.000113	0.000210	1.000000	-0.000171	-0.005969	-0.101246	-0.101060	-0.100806	-0.101468	-0.101740	-0.101851
dayPrecent	-0.154970	-0.000846	-0.000171	1.000000	0.000576	-0.171249	-0.174957	-0.172686	-0.164061	-0.148132	-0.170278
yearPrecent	-0.084550	-0.013170	-0.005969	0.000576	1.000000	0.001950	-0.000029	0.000178	-0.000772	-0.000727	-0.000189
pastET	-0.073074	0.002810	-0.101246	-0.171249	0.001950	1.000000	0.581571	0.541445	0.508681	0.480528	0.613961
energy_0	-0.078214	0.002464	-0.101060	-0.174957	-0.000029	0.581571	1.000000	0.779925	0.660409	0.592972	0.780838
energy_1	-0.080345	0.002604	-0.100806	-0.172686	0.000178	0.541445	0.779925	1.000000	0.779235	0.660638	0.660564
energy_2	-0.082651	0.002753	-0.101468	-0.164061	-0.000772	0.508681	0.660409	0.779235	1.000000	0.781977	0.591699
energy_3	-0.085751	0.002454	-0.101740	-0.148132	-0.000727	0.480528	0.592972	0.660638	0.781977	1.000000	0.542677
energy	-0.077516	0.002171	-0.101851	-0.170278	-0.000189	0.613961	0.780838	0.660564	0.591699	0.542677	1.000000


def exponential_decay(lr0, s):
    def exponential_decay_fn(epoch):
        return lr0 * 0.1**(epoch / s)
    return exponential_decay_fn

exponential_decay_fn = exponential_decay(lr0=0.01, s=20)

lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)
history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])

s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)
learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)
optimizer = keras.optimizers.SGD(learning_rate)


model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(300, activation="elu", kernel_initializer="he_normal"),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(100, activation="elu", kernel_initializer="he_normal"),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(10, activation="softmax")
])

y_probas = np.stack([model(X_test_scaled, training=True)
                     for sample in range(100)])
y_proba = y_probas.mean(axis=0)


# define own layers np for multiplications of data 
class MyMultiLayer(keras.layers.Layer):
    def call(self, X):
        X1, X2 = X
        return [X1 + X2, X1 * X2, X1 / X2]

    def compute_output_shape(self, batch_input_shape):
        b1, b2 = batch_input_shape
        return [b1, b1, b1] # should probably handle broadcasting rules


#read data from csv 
filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)

########################################################################

from pandas import read_csv
from matplotlib import pyplot
from pandas.plotting import autocorrelation_plot
series = read_csv('daily-min-temperatures.csv', header=0, index_col=0)
autocorrelation_plot(series)
pyplot.show()


from pandas import read_csv
from matplotlib import pyplot
from statsmodels.graphics.tsaplots import plot_acf
series = read_csv('daily-min-temperatures.csv', header=0, index_col=0)
plot_acf(series, lags=31)
pyplot.show()


#   NARXRecurrent
input_nodes = 2
hidden_nodes = 2
output_nodes = 2

output_order = 3
incoming_weight_from_output = .6
input_order = 2
incoming_weight_from_input = .4

net = NeuralNet()
net.init_layers(input_nodes, [hidden_nodes], output_nodes,
        NARXRecurrent(
            output_order,
            incoming_weight_from_output,
            input_order,
            incoming_weight_from_input))

net.randomize_network()